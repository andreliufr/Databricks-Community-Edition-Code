{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38fd5633-be5b-447e-89d0-d832647e677a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Databricks code Source : https://docs.databricks.com/en/connect/external-systems/elasticsearch.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c314f2d-055a-4747-9988-6011a2d69814",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Elasticsearch\n",
    "\n",
    "<img src=\"https://static-www.elastic.co/v3/assets/bltefdd0b53724fa2ce/blt5ebe80fb665aef6b/5ea8c8f26b62d4563b6ecec2/brand-elasticsearch-220x130.svg\" width=\"300\">\n",
    "\n",
    "1. Launch a cluster in your workspace or choose an existing cluster.\n",
    "2. Once the new cluster is running, go to the \"Libraries\" tab of that cluster, and click \"Install new\" -> choose \"Maven\" -> enter the maven coordinates `org.elasticsearch:elasticsearch-spark-30_2.12:8.4.3` -> click \"Install\". If running into errors like `org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version` while the ES connection is verified, consider install newer versions that matches your ES service.\n",
    "3. Once the installation has finished, attach this notebook to the cluster, and run write and/or read operations against your Elasticsearch cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a0f18d4-4b89-414a-9a82-60c6d00a3a16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Important**: In the following cells, replace `<ip-address>`, `<port>`, `<ssl>`, `<hostname>` and `<index>` with your Elasticsearch configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "970532f4-49c9-43ee-b1bb-1b60d90110b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Install Elasticseach on Databricks worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a94ccd-34f6-4134-b2e4-7c2041df6bd4",
     "showTitle": true,
     "title": "download key"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpg: cannot open '/dev/tty': No such device or address\n(23) Failed writing body\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "\n",
    "curl -fsSL https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-archive-keyring.gpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7699c51a-ac58-4f2c-ac86-5f5171c235f1",
     "showTitle": true,
     "title": "add source"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deb [signed-by=/usr/share/keyrings/elasticsearch-archive-keyring.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "echo \"deb [signed-by=/usr/share/keyrings/elasticsearch-archive-keyring.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main\" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e25707e-22fa-4510-8452-7e9eb368ba66",
     "showTitle": true,
     "title": "update apt"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://artifacts.elastic.co/packages/8.x/apt stable InRelease [10.4 kB]\nHit:2 https://repos.azul.com/zulu/deb stable InRelease\nHit:3 http://security.ubuntu.com/ubuntu focal-security InRelease\nHit:4 http://archive.ubuntu.com/ubuntu focal InRelease\nHit:5 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\nGet:6 https://artifacts.elastic.co/packages/8.x/apt stable/main amd64 Packages [126 kB]\nGet:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]\nHit:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease\nGet:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [4,435 kB]\nGet:10 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [33.5 kB]\nGet:11 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,558 kB]\nGet:12 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [4,124 kB]\nFetched 10.4 MB in 3s (3,158 kB/s)\nReading package lists...\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "sudo apt-get update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a27be333-ce5b-4713-9907-25baf99af8da",
     "showTitle": true,
     "title": "install with apt"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\nBuilding dependency tree...\nReading state information...\nThe following NEW packages will be installed:\n  elasticsearch\n0 upgraded, 1 newly installed, 0 to remove and 51 not upgraded.\nNeed to get 606 MB of archives.\nAfter this operation, 1,168 MB of additional disk space will be used.\nGet:1 https://artifacts.elastic.co/packages/8.x/apt stable/main amd64 elasticsearch amd64 8.15.1 [606 MB]\ndebconf: delaying package configuration, since apt-utils is not installed\nFetched 606 MB in 20s (29.8 MB/s)\nSelecting previously unselected package elasticsearch.\n(Reading database ... \n(Reading database ... 5%\n(Reading database ... 10%\n(Reading database ... 15%\n(Reading database ... 20%\n(Reading database ... 25%\n(Reading database ... 30%\n(Reading database ... 35%\n(Reading database ... 40%\n(Reading database ... 45%\n(Reading database ... 50%\n(Reading database ... 55%\n(Reading database ... 60%\n(Reading database ... 65%\n(Reading database ... 70%\n(Reading database ... 75%\n(Reading database ... 80%\n(Reading database ... 85%\n(Reading database ... 90%\n(Reading database ... 95%\n(Reading database ... 100%\n(Reading database ... 98394 files and directories currently installed.)\nPreparing to unpack .../elasticsearch_8.15.1_amd64.deb ...\nCreating elasticsearch group... OK\nCreating elasticsearch user... OK\nUnpacking elasticsearch (8.15.1) ...\nSetting up elasticsearch (8.15.1) ...\n--------------------------- Security autoconfiguration information ------------------------------\n\nAuthentication and authorization are enabled.\nTLS for the transport and HTTP layers is enabled and configured.\n\nThe generated password for the elastic built-in superuser is : D5Gro*bR5us2hYt06dXI\n\nIf this node should join an existing cluster, you can reconfigure this with\n'/usr/share/elasticsearch/bin/elasticsearch-reconfigure-node --enrollment-token <token-here>'\nafter creating an enrollment token on your existing cluster.\n\nYou can complete the following actions at any time:\n\nReset the password of the elastic built-in superuser with \n'/usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic'.\n\nGenerate an enrollment token for Kibana instances with \n '/usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana'.\n\nGenerate an enrollment token for Elasticsearch nodes with \n'/usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s node'.\n\n-------------------------------------------------------------------------------------------------\n### NOT starting on installation, please execute the following statements to configure elasticsearch service to start automatically using systemd\n sudo systemctl daemon-reload\n sudo systemctl enable elasticsearch.service\n### You can start elasticsearch service by executing\n sudo systemctl start elasticsearch.service\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "\n",
    "sudo apt-get install elasticsearch\n",
    "\n",
    "#The generated password for the elastic built-in superuser is : D5Gro*bR5us2hYt06dXI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d400eac-58b3-4933-8893-e40b9c8a1ce6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Start Elasticserach service "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bfe7b2c-eb1e-4b93-97c1-3c5054d9c331",
     "showTitle": true,
     "title": "enable elasticsearch"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created symlink /etc/systemd/system/multi-user.target.wants/elasticsearch.service → /lib/systemd/system/elasticsearch.service.\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "\n",
    "sudo systemctl enable elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5462ab0-4e5e-4065-9c3e-7d78bd5c885c",
     "showTitle": true,
     "title": "start elasticsearch"
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "\n",
    "sudo systemctl start elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b730f897-e1fb-432b-8d1d-be186277809d",
     "showTitle": true,
     "title": "check elasticsearch status"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● elasticsearch.service - Elasticsearch\n     Loaded: loaded (/lib/systemd/system/elasticsearch.service; enabled; vendor preset: enabled)\n     Active: active (running) since Mon 2024-09-16 07:41:12 UTC; 38min ago\n       Docs: https://www.elastic.co\n   Main PID: 2893 (java)\n     CGroup: /system.slice/elasticsearch.service\n             ├─2893 /usr/share/elasticsearch/jdk/bin/java -Xms4m -Xmx64m -XX:+UseSerialGC -Dcli.name=server -Dcli.script=/usr/share/elasticsearch/bin/elasticsearch -Dcli.libs=lib/tools/server-cli -Des.path.home=/usr/share/elasticsearch -Des.path.conf=/etc/elasticsearch -Des.distribution.type=deb -cp /usr/share/elasticsearch/lib/*:/usr/share/elasticsearch/lib/cli-launcher/* org.elasticsearch.launcher.CliToolLauncher -p /var/run/elasticsearch/elasticsearch.pid --quiet\n             ├─2964 /usr/share/elasticsearch/jdk/bin/java -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -Djava.security.manager=allow -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Dlog4j2.formatMsgNoLookups=true -Djava.locale.providers=SPI,COMPAT --add-opens=java.base/java.io=org.elasticsearch.preallocate --enable-native-access=org.elasticsearch.nativeaccess,org.apache.lucene.core -XX:ReplayDataFile=/var/log/elasticsearch/replay_pid%p.log -Djava.library.path=/usr/share/elasticsearch/lib/platform/linux-x64:/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib -Djna.library.path=/usr/share/elasticsearch/lib/platform/linux-x64:/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib -Des.distribution.type=deb -XX:+UnlockDiagnosticVMOptions -XX:G1NumCollectionsKeepPinned=10000000 -XX:+UseG1GC -Djava.io.tmpdir=/tmp/elasticsearch-1011582052528151951 --add-modules=jdk.incubator.vector -XX:+HeapDumpOnOutOfMemoryError -XX:+ExitOnOutOfMemoryError -XX:HeapDumpPath=/var/lib/elasticsearch -XX:ErrorFile=/var/log/elasticsearch/hs_err_pid%p.log -Xlog:gc*,gc+age=trace,safepoint:file=/var/log/elasticsearch/gc.log:utctime,level,pid,tags:filecount=32,filesize=64m -Xms7615m -Xmx7615m -XX:MaxDirectMemorySize=3992977408 -XX:G1HeapRegionSize=4m -XX:InitiatingHeapOccupancyPercent=30 -XX:G1ReservePercent=15 --module-path /usr/share/elasticsearch/lib --add-modules=jdk.net --add-modules=ALL-MODULE-PATH -m org.elasticsearch.server/org.elasticsearch.bootstrap.Elasticsearch\n             └─3039 /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/controller\n\nSep 16 07:40:03 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Starting Elasticsearch...\nSep 16 07:40:25 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[2893]: Sep 16, 2024 7:40:25 AM sun.util.locale.provider.LocaleProviderAdapter <clinit>\nSep 16 07:40:25 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[2893]: WARNING: COMPAT locale provider will be removed in a future release\nSep 16 07:41:12 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Started Elasticsearch.\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "\n",
    "sudo systemctl status elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c03b0913-b901-471c-a52c-d8b97cdecd37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description:\tUbuntu 20.04.6 LTS\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "\n",
    "lsb_release -d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad28413d-b264-47e1-a6e7-e673bfb1cf4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-16T07:59:42,023][WARN ][o.e.h.n.Netty4HttpServerTransport] [0916-072530-anrzknxi-10-172-214-67] received plaintext http traffic on an https channel, closing connection Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:37548}\n[2024-09-16T07:59:52,488][WARN ][o.e.h.n.Netty4HttpServerTransport] [0916-072530-anrzknxi-10-172-214-67] received plaintext http traffic on an https channel, closing connection Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:48968}\n[2024-09-16T07:59:55,906][WARN ][o.e.h.n.Netty4HttpServerTransport] [0916-072530-anrzknxi-10-172-214-67] received plaintext http traffic on an https channel, closing connection Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:48972}\n[2024-09-16T07:59:58,867][WARN ][o.e.h.n.Netty4HttpServerTransport] [0916-072530-anrzknxi-10-172-214-67] received plaintext http traffic on an https channel, closing connection Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:48984}\n[2024-09-16T08:00:06,499][WARN ][o.e.h.n.Netty4HttpServerTransport] [0916-072530-anrzknxi-10-172-214-67] received plaintext http traffic on an https channel, closing connection Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:48070}\n[2024-09-16T08:00:29,519][WARN ][o.e.h.n.Netty4HttpServerTransport] [0916-072530-anrzknxi-10-172-214-67] received plaintext http traffic on an https channel, closing connection Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:45398}\n[2024-09-16T08:00:35,336][WARN ][o.e.h.n.Netty4HttpServerTransport] [0916-072530-anrzknxi-10-172-214-67] received plaintext http traffic on an https channel, closing connection Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:45414}\n[2024-09-16T08:00:38,555][WARN ][o.e.h.n.Netty4HttpServerTransport] [0916-072530-anrzknxi-10-172-214-67] received plaintext http traffic on an https channel, closing connection Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:45430}\n[2024-09-16T08:00:54,131][WARN ][o.e.h.n.Netty4HttpServerTransport] [0916-072530-anrzknxi-10-172-214-67] received plaintext http traffic on an https channel, closing connection Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:46034}\n[2024-09-16T08:03:19,721][WARN ][o.e.h.n.Netty4HttpServerTransport] [0916-072530-anrzknxi-10-172-214-67] received plaintext http traffic on an https channel, closing connection Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:37452}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh \n",
    "\n",
    "sudo tail -f /var/log/elasticsearch/elasticsearch.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dfe2019-848f-434f-b485-bcb1b899f8a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Backup elasticsearch.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "858283dc-5d6c-4609-b86e-4d08837e12df",
     "showTitle": true,
     "title": "create backup dir"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/tmp’: File exists\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "\n",
    "mkdir /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de9175ea-1f08-4b07-98ce-0bde460648a5",
     "showTitle": true,
     "title": "read content"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n# Allow HTTP API connections from anywhere\n# Connections are encrypted and require user authentication\nhttp.host: 0.0.0.0\n\n# Allow other nodes to join the cluster from anywhere\n# Connections are encrypted and mutually authenticated\n#transport.host: 0.0.0.0\n\n#----------------------- END SECURITY AUTO CONFIGURATION -------------------------\n"
     ]
    }
   ],
   "source": [
    "%sh tail /etc/elasticsearch/elasticsearch.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fc4c286-dd1b-453c-ae04-42b36776a2c0",
     "showTitle": true,
     "title": "copy to backup"
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "cp /etc/elasticsearch/elasticsearch.yml /tmp/elasticsearch_cp.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c24bfb6-0fc4-4aaf-9e56-066a602f4e22",
     "showTitle": true,
     "title": "create backup from scratch"
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "echo '# ======================== Elasticsearch Configuration =========================\n",
    "#\n",
    "# NOTE: Elasticsearch comes with reasonable defaults for most settings.\n",
    "#       Before you set out to tweak and tune the configuration, make sure you\n",
    "#       understand what are you trying to accomplish and the consequences.\n",
    "#\n",
    "# The primary way of configuring a node is via this file. This template lists\n",
    "# the most important settings you may want to configure for a production cluster.\n",
    "#\n",
    "# Please consult the documentation for further information on configuration options:\n",
    "# https://www.elastic.co/guide/en/elasticsearch/reference/index.html\n",
    "#\n",
    "# ---------------------------------- Cluster -----------------------------------\n",
    "#\n",
    "# Use a descriptive name for your cluster:\n",
    "#\n",
    "#cluster.name: my-application\n",
    "#\n",
    "# ------------------------------------ Node ------------------------------------\n",
    "#\n",
    "# Use a descriptive name for the node:\n",
    "#\n",
    "#node.name: node-1\n",
    "#\n",
    "# Add custom attributes to the node:\n",
    "#\n",
    "#node.attr.rack: r1\n",
    "#\n",
    "# ----------------------------------- Paths ------------------------------------\n",
    "#\n",
    "# Path to directory where to store the data (separate multiple locations by comma):\n",
    "#\n",
    "path.data: /var/lib/elasticsearch\n",
    "#\n",
    "# Path to log files:\n",
    "#\n",
    "path.logs: /var/log/elasticsearch\n",
    "#\n",
    "# ----------------------------------- Memory -----------------------------------\n",
    "#\n",
    "# Lock the memory on startup:\n",
    "#\n",
    "#bootstrap.memory_lock: true\n",
    "#\n",
    "# Make sure that the heap size is set to about half the memory available\n",
    "# on the system and that the owner of the process is allowed to use this\n",
    "# limit.\n",
    "#\n",
    "# Elasticsearch performs poorly when the system is swapping the memory.\n",
    "#\n",
    "# ---------------------------------- Network -----------------------------------\n",
    "#\n",
    "# By default Elasticsearch is only accessible on localhost. Set a different\n",
    "# address here to expose this node on the network:\n",
    "#\n",
    "#network.host: 192.168.0.1\n",
    "#\n",
    "# By default Elasticsearch listens for HTTP traffic on the first free port it\n",
    "# finds starting at 9200. Set a specific HTTP port here:\n",
    "#\n",
    "#http.port: 9200\n",
    "#\n",
    "# For more information, consult the network module documentation.\n",
    "#\n",
    "# --------------------------------- Discovery ----------------------------------\n",
    "#\n",
    "# Pass an initial list of hosts to perform discovery when this node is started:\n",
    "# The default list of hosts is [\"127.0.0.1\", \"[::1]\"]\n",
    "#\n",
    "#discovery.seed_hosts: [\"host1\", \"host2\"]\n",
    "#\n",
    "# Bootstrap the cluster using an initial set of master-eligible nodes:\n",
    "#\n",
    "#cluster.initial_master_nodes: [\"node-1\", \"node-2\"]\n",
    "#\n",
    "# For more information, consult the discovery and cluster formation module documentation.\n",
    "#\n",
    "# ---------------------------------- Various -----------------------------------\n",
    "#\n",
    "# Allow wildcard deletion of indices:\n",
    "#\n",
    "#action.destructive_requires_name: false\n",
    "\n",
    "#----------------------- BEGIN SECURITY AUTO CONFIGURATION -----------------------\n",
    "#\n",
    "# The following settings, TLS certificates, and keys have been automatically      \n",
    "# generated to configure Elasticsearch security features on 16-09-2024 07:39:27\n",
    "#\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Enable security features\n",
    "xpack.security.enabled: false\n",
    "\n",
    "xpack.security.enrollment.enabled: true\n",
    "\n",
    "# Enable encryption for HTTP API client connections, such as Kibana, Logstash, and Agents\n",
    "xpack.security.http.ssl:\n",
    "  enabled: true\n",
    "  keystore.path: certs/http.p12\n",
    "\n",
    "# Enable encryption and mutual authentication between cluster nodes\n",
    "xpack.security.transport.ssl:\n",
    "  enabled: true\n",
    "  verification_mode: certificate\n",
    "  keystore.path: certs/transport.p12\n",
    "  truststore.path: certs/transport.p12\n",
    "# Create a new cluster with the current node only\n",
    "# Additional nodes can still join the cluster later\n",
    "cluster.initial_master_nodes: [\"0916-072530-anrzknxi-10-172-214-67\"]\n",
    "\n",
    "# Allow HTTP API connections from anywhere\n",
    "# Connections are encrypted and require user authentication\n",
    "http.host: 0.0.0.0\n",
    "\n",
    "# Allow other nodes to join the cluster from anywhere\n",
    "# Connections are encrypted and mutually authenticated\n",
    "#transport.host: 0.0.0.0\n",
    "\n",
    "#----------------------- END SECURITY AUTO CONFIGURATION -------------------------\n",
    "' > /tmp/elasticsearch_default.yml\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c29bdbcb-ef7c-4822-9b2e-7b9fffe1d16c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/elasticsearch_cp.yml\n/tmp/elasticsearch_default.yml\n\n/tmp/elasticsearch-1011582052528151951:\ngeoip-databases\njansi-2.4.0-eaf0436a468662be-libjansi.so\njansi-2.4.0-eaf0436a468662be-libjansi.so.lck\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "ls /tmp/e*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72bbf2fa-9628-4861-8c30-833b5a40fcee",
     "showTitle": true,
     "title": "verify content"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ======================== Elasticsearch Configuration =========================\r\n#\r\n# NOTE: Elasticsearch comes with reasonable defaults for most settings.\r\n#       Before you set out to tweak and tune the configuration, make sure you\r\n#       understand what are you trying to accomplish and the consequences.\r\n#\r\n# The primary way of configuring a node is via this file. This template lists\r\n# the most important settings you may want to configure for a production cluster.\r\n#\r\n# Please consult the documentation for further information on configuration options:\r\n# https://www.elastic.co/guide/en/elasticsearch/reference/index.html\r\n#\r\n# ---------------------------------- Cluster -----------------------------------\r\n#\r\n# Use a descriptive name for your cluster:\r\n#\r\n#cluster.name: my-application\r\n#\r\n# ------------------------------------ Node ------------------------------------\r\n#\r\n# Use a descriptive name for the node:\r\n#\r\n#node.name: node-1\r\n#\r\n# Add custom attributes to the node:\r\n#\r\n#node.attr.rack: r1\r\n#\r\n# ----------------------------------- Paths ------------------------------------\r\n#\r\n# Path to directory where to store the data (separate multiple locations by comma):\r\n#\r\npath.data: /var/lib/elasticsearch\r\n#\r\n# Path to log files:\r\n#\r\npath.logs: /var/log/elasticsearch\r\n#\r\n# ----------------------------------- Memory -----------------------------------\r\n#\r\n# Lock the memory on startup:\r\n#\r\n#bootstrap.memory_lock: true\r\n#\r\n# Make sure that the heap size is set to about half the memory available\r\n# on the system and that the owner of the process is allowed to use this\r\n# limit.\r\n#\r\n# Elasticsearch performs poorly when the system is swapping the memory.\r\n#\r\n# ---------------------------------- Network -----------------------------------\r\n#\r\n# By default Elasticsearch is only accessible on localhost. Set a different\r\n# address here to expose this node on the network:\r\n#\r\n#network.host: 192.168.0.1\r\n#\r\n# By default Elasticsearch listens for HTTP traffic on the first free port it\r\n# finds starting at 9200. Set a specific HTTP port here:\r\n#\r\n#http.port: 9200\r\n#\r\n# For more information, consult the network module documentation.\r\n#\r\n# --------------------------------- Discovery ----------------------------------\r\n#\r\n# Pass an initial list of hosts to perform discovery when this node is started:\r\n# The default list of hosts is [\"127.0.0.1\", \"[::1]\"]\r\n#\r\n#discovery.seed_hosts: [\"host1\", \"host2\"]\r\n#\r\n# Bootstrap the cluster using an initial set of master-eligible nodes:\r\n#\r\n#cluster.initial_master_nodes: [\"node-1\", \"node-2\"]\r\n#\r\n# For more information, consult the discovery and cluster formation module documentation.\r\n#\r\n# ---------------------------------- Various -----------------------------------\r\n#\r\n# Allow wildcard deletion of indices:\r\n#\r\n#action.destructive_requires_name: false\r\n\r\n#----------------------- BEGIN SECURITY AUTO CONFIGURATION -----------------------\r\n#\r\n# The following settings, TLS certificates, and keys have been automatically      \r\n# generated to configure Elasticsearch security features on 16-09-2024 07:39:27\r\n#\r\n# --------------------------------------------------------------------------------\r\n\r\n# Enable security features\r\nxpack.security.enabled: true\r\n\r\nxpack.security.enrollment.enabled: true\r\n\r\n# Enable encryption for HTTP API client connections, such as Kibana, Logstash, and Agents\r\nxpack.security.http.ssl:\r\n  enabled: true\r\n  keystore.path: certs/http.p12\r\n\r\n# Enable encryption and mutual authentication between cluster nodes\r\nxpack.security.transport.ssl:\r\n  enabled: true\r\n  verification_mode: certificate\r\n  keystore.path: certs/transport.p12\r\n  truststore.path: certs/transport.p12\r\n# Create a new cluster with the current node only\r\n# Additional nodes can still join the cluster later\r\ncluster.initial_master_nodes: [\"0916-072530-anrzknxi-10-172-214-67\"]\r\n\r\n# Allow HTTP API connections from anywhere\r\n# Connections are encrypted and require user authentication\r\nhttp.host: 0.0.0.0\r\n\r\n# Allow other nodes to join the cluster from anywhere\r\n# Connections are encrypted and mutually authenticated\r\n#transport.host: 0.0.0.0\r\n\r\n#----------------------- END SECURITY AUTO CONFIGURATION -------------------------\r\n\r\n"
     ]
    }
   ],
   "source": [
    "cat /tmp/elasticsearch_default.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1883eb4-7a90-4d53-ae59-13cca28902af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create new elasticsearch.yml without SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f48c156-18b3-415f-97eb-91289a5b5ec2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "#xpack.security.enabled set to false\n",
    "\n",
    "echo '\n",
    "# ======================== Elasticsearch Configuration =========================\n",
    "#\n",
    "# NOTE: Elasticsearch comes with reasonable defaults for most settings.\n",
    "#       Before you set out to tweak and tune the configuration, make sure you\n",
    "#       understand what are you trying to accomplish and the consequences.\n",
    "#\n",
    "# The primary way of configuring a node is via this file. This template lists\n",
    "# the most important settings you may want to configure for a production cluster.\n",
    "#\n",
    "# Please consult the documentation for further information on configuration options:\n",
    "# https://www.elastic.co/guide/en/elasticsearch/reference/index.html\n",
    "#\n",
    "# ---------------------------------- Cluster -----------------------------------\n",
    "#\n",
    "# Use a descriptive name for your cluster:\n",
    "#\n",
    "#cluster.name: my-application\n",
    "#\n",
    "# ------------------------------------ Node ------------------------------------\n",
    "#\n",
    "# Use a descriptive name for the node:\n",
    "#\n",
    "#node.name: node-1\n",
    "#\n",
    "# Add custom attributes to the node:\n",
    "#\n",
    "#node.attr.rack: r1\n",
    "#\n",
    "# ----------------------------------- Paths ------------------------------------\n",
    "#\n",
    "# Path to directory where to store the data (separate multiple locations by comma):\n",
    "#\n",
    "path.data: /var/lib/elasticsearch\n",
    "#\n",
    "# Path to log files:\n",
    "#\n",
    "path.logs: /var/log/elasticsearch\n",
    "#\n",
    "# ----------------------------------- Memory -----------------------------------\n",
    "#\n",
    "# Lock the memory on startup:\n",
    "#\n",
    "#bootstrap.memory_lock: true\n",
    "#\n",
    "# Make sure that the heap size is set to about half the memory available\n",
    "# on the system and that the owner of the process is allowed to use this\n",
    "# limit.\n",
    "#\n",
    "# Elasticsearch performs poorly when the system is swapping the memory.\n",
    "#\n",
    "# ---------------------------------- Network -----------------------------------\n",
    "#\n",
    "# By default Elasticsearch is only accessible on localhost. Set a different\n",
    "# address here to expose this node on the network:\n",
    "#\n",
    "#network.host: 192.168.0.1\n",
    "#\n",
    "# By default Elasticsearch listens for HTTP traffic on the first free port it\n",
    "# finds starting at 9200. Set a specific HTTP port here:\n",
    "#\n",
    "http.port: 9200\n",
    "#\n",
    "# For more information, consult the network module documentation.\n",
    "#\n",
    "# --------------------------------- Discovery ----------------------------------\n",
    "#\n",
    "# Pass an initial list of hosts to perform discovery when this node is started:\n",
    "# The default list of hosts is [\"127.0.0.1\", \"[::1]\"]\n",
    "#\n",
    "#discovery.seed_hosts: [\"host1\", \"host2\"]\n",
    "#\n",
    "# Bootstrap the cluster using an initial set of master-eligible nodes:\n",
    "#\n",
    "#cluster.initial_master_nodes: [\"node-1\", \"node-2\"]\n",
    "#\n",
    "# For more information, consult the discovery and cluster formation module documentation.\n",
    "#\n",
    "# ---------------------------------- Various -----------------------------------\n",
    "#\n",
    "# Allow wildcard deletion of indices:\n",
    "#\n",
    "#action.destructive_requires_name: false\n",
    "\n",
    "#----------------------- BEGIN SECURITY AUTO CONFIGURATION -----------------------\n",
    "#\n",
    "# The following settings, TLS certificates, and keys have been automatically      \n",
    "# generated to configure Elasticsearch security features on 16-09-2024 07:39:27\n",
    "#\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Enable security features\n",
    "xpack.security.enabled: false\n",
    "\n",
    "xpack.security.enrollment.enabled: true\n",
    "\n",
    "# Enable encryption for HTTP API client connections, such as Kibana, Logstash, and Agents\n",
    "xpack.security.http.ssl:\n",
    "  enabled: true\n",
    "  keystore.path: certs/http.p12\n",
    "\n",
    "# Enable encryption and mutual authentication between cluster nodes\n",
    "xpack.security.transport.ssl:\n",
    "  enabled: true\n",
    "  verification_mode: certificate\n",
    "  keystore.path: certs/transport.p12\n",
    "  truststore.path: certs/transport.p12\n",
    "# Create a new cluster with the current node only\n",
    "# Additional nodes can still join the cluster later\n",
    "cluster.initial_master_nodes: [\"0916-072530-anrzknxi-10-172-214-67\"]\n",
    "\n",
    "# Allow HTTP API connections from anywhere\n",
    "# Connections are encrypted and require user authentication\n",
    "http.host: 0.0.0.0\n",
    "\n",
    "# Allow other nodes to join the cluster from anywhere\n",
    "# Connections are encrypted and mutually authenticated\n",
    "#transport.host: 0.0.0.0\n",
    "\n",
    "#----------------------- END SECURITY AUTO CONFIGURATION -------------------------\n",
    "' > /tmp/elasticsearch_new.yml\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc67f8da-018f-4f3c-8946-1677dfdc0875",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n# ======================== Elasticsearch Configuration =========================\n#\n# NOTE: Elasticsearch comes with reasonable defaults for most settings.\n#       Before you set out to tweak and tune the configuration, make sure you\n#       understand what are you trying to accomplish and the consequences.\n#\n# The primary way of configuring a node is via this file. This template lists\n# the most important settings you may want to configure for a production cluster.\n#\n# Please consult the documentation for further information on configuration options:\n# https://www.elastic.co/guide/en/elasticsearch/reference/index.html\n#\n# ---------------------------------- Cluster -----------------------------------\n#\n# Use a descriptive name for your cluster:\n#\n#cluster.name: my-application\n#\n# ------------------------------------ Node ------------------------------------\n#\n# Use a descriptive name for the node:\n#\n#node.name: node-1\n#\n# Add custom attributes to the node:\n#\n#node.attr.rack: r1\n#\n# ----------------------------------- Paths ------------------------------------\n#\n# Path to directory where to store the data (separate multiple locations by comma):\n#\npath.data: /var/lib/elasticsearch\n#\n# Path to log files:\n#\npath.logs: /var/log/elasticsearch\n#\n# ----------------------------------- Memory -----------------------------------\n#\n# Lock the memory on startup:\n#\n#bootstrap.memory_lock: true\n#\n# Make sure that the heap size is set to about half the memory available\n# on the system and that the owner of the process is allowed to use this\n# limit.\n#\n# Elasticsearch performs poorly when the system is swapping the memory.\n#\n# ---------------------------------- Network -----------------------------------\n#\n# By default Elasticsearch is only accessible on localhost. Set a different\n# address here to expose this node on the network:\n#\n#network.host: 192.168.0.1\n#\n# By default Elasticsearch listens for HTTP traffic on the first free port it\n# finds starting at 9200. Set a specific HTTP port here:\n#\nhttp.port: 9200\n#\n# For more information, consult the network module documentation.\n#\n# --------------------------------- Discovery ----------------------------------\n#\n# Pass an initial list of hosts to perform discovery when this node is started:\n# The default list of hosts is [\"127.0.0.1\", \"[::1]\"]\n#\n#discovery.seed_hosts: [\"host1\", \"host2\"]\n#\n# Bootstrap the cluster using an initial set of master-eligible nodes:\n#\n#cluster.initial_master_nodes: [\"node-1\", \"node-2\"]\n#\n# For more information, consult the discovery and cluster formation module documentation.\n#\n# ---------------------------------- Various -----------------------------------\n#\n# Allow wildcard deletion of indices:\n#\n#action.destructive_requires_name: false\n\n#----------------------- BEGIN SECURITY AUTO CONFIGURATION -----------------------\n#\n# The following settings, TLS certificates, and keys have been automatically      \n# generated to configure Elasticsearch security features on 16-09-2024 07:39:27\n#\n# --------------------------------------------------------------------------------\n\n# Enable security features\nxpack.security.enabled: false\n\nxpack.security.enrollment.enabled: true\n\n# Enable encryption for HTTP API client connections, such as Kibana, Logstash, and Agents\nxpack.security.http.ssl:\n  enabled: true\n  keystore.path: certs/http.p12\n\n# Enable encryption and mutual authentication between cluster nodes\nxpack.security.transport.ssl:\n  enabled: true\n  verification_mode: certificate\n  keystore.path: certs/transport.p12\n  truststore.path: certs/transport.p12\n# Create a new cluster with the current node only\n# Additional nodes can still join the cluster later\ncluster.initial_master_nodes: [\"0916-072530-anrzknxi-10-172-214-67\"]\n\n# Allow HTTP API connections from anywhere\n# Connections are encrypted and require user authentication\nhttp.host: 0.0.0.0\n\n# Allow other nodes to join the cluster from anywhere\n# Connections are encrypted and mutually authenticated\n#transport.host: 0.0.0.0\n\n#----------------------- END SECURITY AUTO CONFIGURATION -------------------------\n\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "\n",
    "cat /tmp/elasticsearch_new.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42d4c2a8-bd17-4e20-b2ae-dc8a6d3bbf87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Replace elasticsearch.yml and restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a41d5701-4676-4e9d-89de-6d75c1f8097b",
     "showTitle": true,
     "title": "delete yml file"
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "sudo rm /etc/elasticsearch/elasticsearch.yml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fddd7938-6f95-42f3-bba5-f37f84a035c7",
     "showTitle": true,
     "title": "copy yml file"
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "sudo cp /tmp/elasticsearch_new.yml /etc/elasticsearch/elasticsearch.yml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d51c61c-acc1-4a5c-9c5b-ed1d6d82c8f9",
     "showTitle": true,
     "title": "verify yml content"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n# ======================== Elasticsearch Configuration =========================\n#\n# NOTE: Elasticsearch comes with reasonable defaults for most settings.\n#       Before you set out to tweak and tune the configuration, make sure you\n#       understand what are you trying to accomplish and the consequences.\n#\n# The primary way of configuring a node is via this file. This template lists\n# the most important settings you may want to configure for a production cluster.\n#\n# Please consult the documentation for further information on configuration options:\n# https://www.elastic.co/guide/en/elasticsearch/reference/index.html\n#\n# ---------------------------------- Cluster -----------------------------------\n#\n# Use a descriptive name for your cluster:\n#\n#cluster.name: my-application\n#\n# ------------------------------------ Node ------------------------------------\n#\n# Use a descriptive name for the node:\n#\n#node.name: node-1\n#\n# Add custom attributes to the node:\n#\n#node.attr.rack: r1\n#\n# ----------------------------------- Paths ------------------------------------\n#\n# Path to directory where to store the data (separate multiple locations by comma):\n#\npath.data: /var/lib/elasticsearch\n#\n# Path to log files:\n#\npath.logs: /var/log/elasticsearch\n#\n# ----------------------------------- Memory -----------------------------------\n#\n# Lock the memory on startup:\n#\n#bootstrap.memory_lock: true\n#\n# Make sure that the heap size is set to about half the memory available\n# on the system and that the owner of the process is allowed to use this\n# limit.\n#\n# Elasticsearch performs poorly when the system is swapping the memory.\n#\n# ---------------------------------- Network -----------------------------------\n#\n# By default Elasticsearch is only accessible on localhost. Set a different\n# address here to expose this node on the network:\n#\n#network.host: 192.168.0.1\n#\n# By default Elasticsearch listens for HTTP traffic on the first free port it\n# finds starting at 9200. Set a specific HTTP port here:\n#\nhttp.port: 9200\n#\n# For more information, consult the network module documentation.\n#\n# --------------------------------- Discovery ----------------------------------\n#\n# Pass an initial list of hosts to perform discovery when this node is started:\n# The default list of hosts is [\"127.0.0.1\", \"[::1]\"]\n#\n#discovery.seed_hosts: [\"host1\", \"host2\"]\n#\n# Bootstrap the cluster using an initial set of master-eligible nodes:\n#\n#cluster.initial_master_nodes: [\"node-1\", \"node-2\"]\n#\n# For more information, consult the discovery and cluster formation module documentation.\n#\n# ---------------------------------- Various -----------------------------------\n#\n# Allow wildcard deletion of indices:\n#\n#action.destructive_requires_name: false\n\n#----------------------- BEGIN SECURITY AUTO CONFIGURATION -----------------------\n#\n# The following settings, TLS certificates, and keys have been automatically      \n# generated to configure Elasticsearch security features on 16-09-2024 07:39:27\n#\n# --------------------------------------------------------------------------------\n\n# Enable security features\nxpack.security.enabled: false\n\nxpack.security.enrollment.enabled: true\n\n# Enable encryption for HTTP API client connections, such as Kibana, Logstash, and Agents\nxpack.security.http.ssl:\n  enabled: true\n  keystore.path: certs/http.p12\n\n# Enable encryption and mutual authentication between cluster nodes\nxpack.security.transport.ssl:\n  enabled: true\n  verification_mode: certificate\n  keystore.path: certs/transport.p12\n  truststore.path: certs/transport.p12\n# Create a new cluster with the current node only\n# Additional nodes can still join the cluster later\ncluster.initial_master_nodes: [\"0916-072530-anrzknxi-10-172-214-67\"]\n\n# Allow HTTP API connections from anywhere\n# Connections are encrypted and require user authentication\nhttp.host: 0.0.0.0\n\n# Allow other nodes to join the cluster from anywhere\n# Connections are encrypted and mutually authenticated\n#transport.host: 0.0.0.0\n\n#----------------------- END SECURITY AUTO CONFIGURATION -------------------------\n\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "\n",
    "cat /etc/elasticsearch/elasticsearch.yml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32da4050-58e5-4fa6-80f4-803e874bae7e",
     "showTitle": true,
     "title": "restart service"
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "\n",
    "sudo systemctl restart elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee4f03f1-92f9-4be1-acf8-f727d6ef0b7b",
     "showTitle": true,
     "title": "check service status"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● elasticsearch.service - Elasticsearch\n     Loaded: loaded (/lib/systemd/system/elasticsearch.service; enabled; vendor preset: enabled)\n     Active: active (running) since Mon 2024-09-16 09:01:04 UTC; 1min 18s ago\n       Docs: https://www.elastic.co\n   Main PID: 11953 (java)\n     CGroup: /system.slice/elasticsearch.service\n             ├─11953 /usr/share/elasticsearch/jdk/bin/java -Xms4m -Xmx64m -XX:+UseSerialGC -Dcli.name=server -Dcli.script=/usr/share/elasticsearch/bin/elasticsearch -Dcli.libs=lib/tools/server-cli -Des.path.home=/usr/share/elasticsearch -Des.path.conf=/etc/elasticsearch -Des.distribution.type=deb -cp /usr/share/elasticsearch/lib/*:/usr/share/elasticsearch/lib/cli-launcher/* org.elasticsearch.launcher.CliToolLauncher -p /var/run/elasticsearch/elasticsearch.pid --quiet\n             ├─12043 /usr/share/elasticsearch/jdk/bin/java -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -Djava.security.manager=allow -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Dlog4j2.formatMsgNoLookups=true -Djava.locale.providers=SPI,COMPAT --add-opens=java.base/java.io=org.elasticsearch.preallocate --enable-native-access=org.elasticsearch.nativeaccess,org.apache.lucene.core -XX:ReplayDataFile=/var/log/elasticsearch/replay_pid%p.log -Djava.library.path=/usr/share/elasticsearch/lib/platform/linux-x64:/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib -Djna.library.path=/usr/share/elasticsearch/lib/platform/linux-x64:/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib -Des.distribution.type=deb -XX:+UnlockDiagnosticVMOptions -XX:G1NumCollectionsKeepPinned=10000000 -XX:+UseG1GC -Djava.io.tmpdir=/tmp/elasticsearch-7277450038755883631 --add-modules=jdk.incubator.vector -XX:+HeapDumpOnOutOfMemoryError -XX:+ExitOnOutOfMemoryError -XX:HeapDumpPath=/var/lib/elasticsearch -XX:ErrorFile=/var/log/elasticsearch/hs_err_pid%p.log -Xlog:gc*,gc+age=trace,safepoint:file=/var/log/elasticsearch/gc.log:utctime,level,pid,tags:filecount=32,filesize=64m -Xms7615m -Xmx7615m -XX:MaxDirectMemorySize=3992977408 -XX:G1HeapRegionSize=4m -XX:InitiatingHeapOccupancyPercent=30 -XX:G1ReservePercent=15 --module-path /usr/share/elasticsearch/lib --add-modules=jdk.net --add-modules=ALL-MODULE-PATH -m org.elasticsearch.server/org.elasticsearch.bootstrap.Elasticsearch\n             └─12069 /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/controller\n\nSep 16 09:00:08 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Starting Elasticsearch...\nSep 16 09:00:23 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[11953]: Sep 16, 2024 9:00:23 AM sun.util.locale.provider.LocaleProviderAdapter <clinit>\nSep 16 09:00:23 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[11953]: WARNING: COMPAT locale provider will be removed in a future release\nSep 16 09:01:04 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Started Elasticsearch.\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "\n",
    "sudo systemctl status elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d386422f-45b3-41e8-be7a-47da1062f5e1",
     "showTitle": true,
     "title": "check journal for errors"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Logs begin at Mon 2024-09-16 07:26:22 UTC, end at Mon 2024-09-16 09:01:05 UTC. --\nSep 16 07:36:32 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: 216.31.17.12 local addr 10.172.214.67 -> <null>\nSep 16 07:36:33 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:34 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:35 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:36 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:36 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: 173.255.232.93 local addr 10.172.214.67 -> <null>\nSep 16 07:36:37 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:37 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: 207.246.65.226 local addr 10.172.214.67 -> <null>\nSep 16 07:36:38 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:39 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:40 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:40 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: 74.208.25.46 local addr 10.172.214.67 -> <null>\nSep 16 07:36:41 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:41 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: 134.215.155.177 local addr 10.172.214.67 -> <null>\nSep 16 07:36:42 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:42 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: 208.67.72.43 local addr 10.172.214.67 -> <null>\nSep 16 07:36:43 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:44 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:45 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:45 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: 135.148.100.14 local addr 10.172.214.67 -> <null>\nSep 16 07:36:46 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:47 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:48 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:49 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:50 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:51 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:52 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:53 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:54 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:55 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:56 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:57 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:58 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:36:59 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:37:00 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:37:01 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:37:02 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:37:03 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:37:04 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:37:05 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:37:06 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:37:07 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:37:08 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: adj_systime: Operation not permitted\nSep 16 07:37:21 0916-072530-anrzknxi-10-172-214-67 sudo[1945]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/apt-get install elasticsearch\nSep 16 07:37:21 0916-072530-anrzknxi-10-172-214-67 sudo[1945]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 07:37:23 0916-072530-anrzknxi-10-172-214-67 sudo[1945]: pam_unix(sudo:session): session closed for user root\nSep 16 07:37:45 0916-072530-anrzknxi-10-172-214-67 sudo[1984]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/gpg --dearmor -o /usr/share/keyrings/elasticsearch-archive-keyring.gpg\nSep 16 07:37:45 0916-072530-anrzknxi-10-172-214-67 sudo[1984]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 07:37:45 0916-072530-anrzknxi-10-172-214-67 sudo[1984]: pam_unix(sudo:session): session closed for user root\nSep 16 07:37:49 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: 167.248.62.201 local addr 10.172.214.67 -> <null>\nSep 16 07:37:54 0916-072530-anrzknxi-10-172-214-67 sudo[1989]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/gpg --dearmor -o /usr/share/keyrings/elasticsearch-archive-keyring.gpg\nSep 16 07:37:54 0916-072530-anrzknxi-10-172-214-67 sudo[1989]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 07:37:54 0916-072530-anrzknxi-10-172-214-67 sudo[1989]: pam_unix(sudo:session): session closed for user root\nSep 16 07:38:06 0916-072530-anrzknxi-10-172-214-67 sudo[2014]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/tee /etc/apt/sources.list.d/elastic-8.x.list\nSep 16 07:38:06 0916-072530-anrzknxi-10-172-214-67 sudo[2014]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 07:38:06 0916-072530-anrzknxi-10-172-214-67 sudo[2014]: pam_unix(sudo:session): session closed for user root\nSep 16 07:38:23 0916-072530-anrzknxi-10-172-214-67 sudo[2066]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/apt-get update\nSep 16 07:38:23 0916-072530-anrzknxi-10-172-214-67 sudo[2066]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 07:38:26 0916-072530-anrzknxi-10-172-214-67 dbus-daemon[58]: [system] Activating via systemd: service name='org.freedesktop.PackageKit' unit='packagekit.service' requested by ':1.5' (uid=0 pid=2492 comm=\"/usr/bin/gdbus call --system --dest org.freedeskto\" label=\"lxc-container-default (enforce)\")\nSep 16 07:38:26 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Starting PackageKit Daemon...\n-- Subject: A start job for unit packagekit.service has begun execution\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit packagekit.service has begun execution.\n-- \n-- The job identifier is 1217.\nSep 16 07:38:26 0916-072530-anrzknxi-10-172-214-67 PackageKit[2495]: daemon start\nSep 16 07:38:26 0916-072530-anrzknxi-10-172-214-67 dbus-daemon[58]: [system] Activating via systemd: service name='org.freedesktop.PolicyKit1' unit='polkit.service' requested by ':1.6' (uid=0 pid=2495 comm=\"/usr/lib/packagekit/packagekitd \" label=\"lxc-container-default (enforce)\")\nSep 16 07:38:26 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Starting Authorization Manager...\n-- Subject: A start job for unit polkit.service has begun execution\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit polkit.service has begun execution.\n-- \n-- The job identifier is 1261.\nSep 16 07:38:26 0916-072530-anrzknxi-10-172-214-67 polkitd[2499]: started daemon version 0.105 using authority implementation `local' version `0.105'\nSep 16 07:38:26 0916-072530-anrzknxi-10-172-214-67 dbus-daemon[58]: [system] Successfully activated service 'org.freedesktop.PolicyKit1'\nSep 16 07:38:26 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Started Authorization Manager.\n-- Subject: A start job for unit polkit.service has finished successfully\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit polkit.service has finished successfully.\n-- \n-- The job identifier is 1261.\nSep 16 07:38:26 0916-072530-anrzknxi-10-172-214-67 dbus-daemon[58]: [system] Successfully activated service 'org.freedesktop.PackageKit'\nSep 16 07:38:26 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Started PackageKit Daemon.\n-- Subject: A start job for unit packagekit.service has finished successfully\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit packagekit.service has finished successfully.\n-- \n-- The job identifier is 1217.\nSep 16 07:38:28 0916-072530-anrzknxi-10-172-214-67 sudo[2066]: pam_unix(sudo:session): session closed for user root\nSep 16 07:38:41 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: 161.35.230.200 local addr 10.172.214.67 -> <null>\nSep 16 07:38:44 0916-072530-anrzknxi-10-172-214-67 sudo[2539]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/apt-get install elasticsearch\nSep 16 07:38:44 0916-072530-anrzknxi-10-172-214-67 sudo[2539]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 07:38:55 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: 208.113.130.146 local addr 10.172.214.67 -> <null>\nSep 16 07:39:01 0916-072530-anrzknxi-10-172-214-67 CRON[2556]: pam_unix(cron:session): session opened for user root by (uid=0)\nSep 16 07:39:01 0916-072530-anrzknxi-10-172-214-67 CRON[2557]: (root) CMD (  [ -x /usr/lib/php/sessionclean ] && if [ ! -d /run/systemd/system ]; then /usr/lib/php/sessionclean; fi)\nSep 16 07:39:01 0916-072530-anrzknxi-10-172-214-67 CRON[2556]: pam_unix(cron:session): session closed for user root\nSep 16 07:39:07 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Starting Clean php session files...\n-- Subject: A start job for unit phpsessionclean.service has begun execution\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit phpsessionclean.service has begun execution.\n-- \n-- The job identifier is 1305.\nSep 16 07:39:07 0916-072530-anrzknxi-10-172-214-67 systemd[1]: phpsessionclean.service: Succeeded.\n-- Subject: Unit succeeded\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- The unit phpsessionclean.service has successfully entered the 'dead' state.\nSep 16 07:39:07 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Finished Clean php session files.\n-- Subject: A start job for unit phpsessionclean.service has finished successfully\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit phpsessionclean.service has finished successfully.\n-- \n-- The job identifier is 1305.\nSep 16 07:39:08 0916-072530-anrzknxi-10-172-214-67 groupadd[2631]: group added to /etc/group: name=elasticsearch, GID=111\nSep 16 07:39:08 0916-072530-anrzknxi-10-172-214-67 groupadd[2631]: group added to /etc/gshadow: name=elasticsearch\nSep 16 07:39:08 0916-072530-anrzknxi-10-172-214-67 groupadd[2631]: new group: name=elasticsearch, GID=111\nSep 16 07:39:08 0916-072530-anrzknxi-10-172-214-67 useradd[2643]: new user: name=elasticsearch, UID=108, GID=111, home=/nonexistent, shell=/bin/false, from=none\nSep 16 07:39:08 0916-072530-anrzknxi-10-172-214-67 usermod[2651]: change user 'elasticsearch' password\nSep 16 07:39:08 0916-072530-anrzknxi-10-172-214-67 chage[2658]: changed password expiry for elasticsearch\nSep 16 07:39:24 0916-072530-anrzknxi-10-172-214-67 systemd[1]: systemd-sysctl.service: Succeeded.\n-- Subject: Unit succeeded\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- The unit systemd-sysctl.service has successfully entered the 'dead' state.\nSep 16 07:39:24 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Stopped Apply Kernel Variables.\n-- Subject: A stop job for unit systemd-sysctl.service has finished\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A stop job for unit systemd-sysctl.service has finished.\n-- \n-- The job identifier is 1434 and the job result is done.\nSep 16 07:39:24 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Stopping Apply Kernel Variables...\n-- Subject: A stop job for unit systemd-sysctl.service has begun execution\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A stop job for unit systemd-sysctl.service has begun execution.\n-- \n-- The job identifier is 1434.\nSep 16 07:39:24 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Starting Apply Kernel Variables...\n-- Subject: A start job for unit systemd-sysctl.service has begun execution\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit systemd-sysctl.service has begun execution.\n-- \n-- The job identifier is 1434.\nSep 16 07:39:24 0916-072530-anrzknxi-10-172-214-67 systemd-sysctl[2721]: Not setting net/ipv4/conf/all/promote_secondaries (explicit setting exists).\nSep 16 07:39:24 0916-072530-anrzknxi-10-172-214-67 systemd-sysctl[2721]: Not setting net/ipv4/conf/default/promote_secondaries (explicit setting exists).\nSep 16 07:39:24 0916-072530-anrzknxi-10-172-214-67 systemd-sysctl[2721]: Couldn't write 'fq_codel' to 'net/core/default_qdisc', ignoring: No such file or directory\nSep 16 07:39:24 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Finished Apply Kernel Variables.\n-- Subject: A start job for unit systemd-sysctl.service has finished successfully\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit systemd-sysctl.service has finished successfully.\n-- \n-- The job identifier is 1434.\nSep 16 07:39:35 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: 185.125.190.58 local addr 10.172.214.67 -> <null>\nSep 16 07:39:42 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: 185.125.190.56 local addr 10.172.214.67 -> <null>\nSep 16 07:39:48 0916-072530-anrzknxi-10-172-214-67 sudo[2539]: pam_unix(sudo:session): session closed for user root\nSep 16 07:40:01 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: 137.190.2.4 local addr 10.172.214.67 -> <null>\nSep 16 07:40:02 0916-072530-anrzknxi-10-172-214-67 sudo[2891]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/systemctl start elasticsearch\nSep 16 07:40:02 0916-072530-anrzknxi-10-172-214-67 sudo[2891]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 07:40:03 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Starting Elasticsearch...\n-- Subject: A start job for unit elasticsearch.service has begun execution\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit elasticsearch.service has begun execution.\n-- \n-- The job identifier is 1438.\nSep 16 07:40:25 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[2893]: Sep 16, 2024 7:40:25 AM sun.util.locale.provider.LocaleProviderAdapter <clinit>\nSep 16 07:40:25 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[2893]: WARNING: COMPAT locale provider will be removed in a future release\nSep 16 07:41:12 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Started Elasticsearch.\n-- Subject: A start job for unit elasticsearch.service has finished successfully\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit elasticsearch.service has finished successfully.\n-- \n-- The job identifier is 1438.\nSep 16 07:41:12 0916-072530-anrzknxi-10-172-214-67 sudo[2891]: pam_unix(sudo:session): session closed for user root\nSep 16 07:41:13 0916-072530-anrzknxi-10-172-214-67 sudo[3146]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/systemctl status elasticsearch\nSep 16 07:41:13 0916-072530-anrzknxi-10-172-214-67 sudo[3146]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 07:41:13 0916-072530-anrzknxi-10-172-214-67 sudo[3146]: pam_unix(sudo:session): session closed for user root\nSep 16 07:41:27 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Starting Cleanup of Temporary Directories...\n-- Subject: A start job for unit systemd-tmpfiles-clean.service has begun execution\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit systemd-tmpfiles-clean.service has begun execution.\n-- \n-- The job identifier is 1568.\nSep 16 07:41:27 0916-072530-anrzknxi-10-172-214-67 systemd[1]: systemd-tmpfiles-clean.service: Succeeded.\n-- Subject: Unit succeeded\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- The unit systemd-tmpfiles-clean.service has successfully entered the 'dead' state.\nSep 16 07:41:27 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Finished Cleanup of Temporary Directories.\n-- Subject: A start job for unit systemd-tmpfiles-clean.service has finished successfully\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit systemd-tmpfiles-clean.service has finished successfully.\n-- \n-- The job identifier is 1568.\nSep 16 07:44:35 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: local_clock: ntp_loopfilter.c line 818: ntp_adjtime: Operation not permitted\nSep 16 07:44:35 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: local_clock: ntp_adjtime(TAI) failed: Operation not permitted\nSep 16 07:44:35 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: local_clock: ntp_loopfilter.c line 848: ntp_adjtime: Operation not permitted\nSep 16 07:44:56 0916-072530-anrzknxi-10-172-214-67 PackageKit[2495]: daemon quit\nSep 16 07:44:56 0916-072530-anrzknxi-10-172-214-67 systemd[1]: packagekit.service: Succeeded.\n-- Subject: Unit succeeded\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- The unit packagekit.service has successfully entered the 'dead' state.\nSep 16 07:45:01 0916-072530-anrzknxi-10-172-214-67 CRON[3557]: pam_unix(cron:session): session opened for user root by (uid=0)\nSep 16 07:45:01 0916-072530-anrzknxi-10-172-214-67 CRON[3559]: (root) CMD (/databricks/spark/scripts/ganglia/save_snapshot.sh)\nSep 16 07:45:01 0916-072530-anrzknxi-10-172-214-67 CRON[3556]: pam_unix(cron:session): session opened for user root by (uid=0)\nSep 16 07:45:01 0916-072530-anrzknxi-10-172-214-67 CRON[3562]: (root) CMD (command -v debian-sa1 > /dev/null && debian-sa1 1 1)\nSep 16 07:45:01 0916-072530-anrzknxi-10-172-214-67 CRON[3556]: pam_unix(cron:session): session closed for user root\nSep 16 07:45:01 0916-072530-anrzknxi-10-172-214-67 CRON[3557]: pam_unix(cron:session): session closed for user root\nSep 16 07:46:52 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: local_clock: ntp_loopfilter.c line 818: ntp_adjtime: Operation not permitted\nSep 16 07:55:01 0916-072530-anrzknxi-10-172-214-67 CRON[4606]: pam_unix(cron:session): session opened for user root by (uid=0)\nSep 16 07:55:01 0916-072530-anrzknxi-10-172-214-67 CRON[4607]: (root) CMD (command -v debian-sa1 > /dev/null && debian-sa1 1 1)\nSep 16 07:55:01 0916-072530-anrzknxi-10-172-214-67 CRON[4606]: pam_unix(cron:session): session closed for user root\nSep 16 07:55:53 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: local_clock: ntp_loopfilter.c line 818: ntp_adjtime: Operation not permitted\nSep 16 07:58:04 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: local_clock: ntp_loopfilter.c line 818: ntp_adjtime: Operation not permitted\nSep 16 08:00:01 0916-072530-anrzknxi-10-172-214-67 CRON[5127]: pam_unix(cron:session): session opened for user root by (uid=0)\nSep 16 08:00:01 0916-072530-anrzknxi-10-172-214-67 CRON[5126]: pam_unix(cron:session): session opened for user root by (uid=0)\nSep 16 08:00:01 0916-072530-anrzknxi-10-172-214-67 CRON[5129]: (root) CMD (/databricks/spark/scripts/ganglia/save_snapshot.sh)\nSep 16 08:00:01 0916-072530-anrzknxi-10-172-214-67 CRON[5130]: (root) CMD (/databricks/spark/scripts/ganglia/remove_old_ganglia_rrds.sh 24 &> /var/log/rrd_cleanup.log)\nSep 16 08:00:01 0916-072530-anrzknxi-10-172-214-67 CRON[5126]: pam_unix(cron:session): session closed for user root\nSep 16 08:00:01 0916-072530-anrzknxi-10-172-214-67 CRON[5127]: pam_unix(cron:session): session closed for user root\nSep 16 08:00:10 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: local_clock: ntp_loopfilter.c line 818: ntp_adjtime: Operation not permitted\nSep 16 08:04:41 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: local_clock: ntp_loopfilter.c line 818: ntp_adjtime: Operation not permitted\nSep 16 08:04:55 0916-072530-anrzknxi-10-172-214-67 sudo[5668]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/systemctl status elasticsearch\nSep 16 08:04:55 0916-072530-anrzknxi-10-172-214-67 sudo[5668]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 08:04:55 0916-072530-anrzknxi-10-172-214-67 sudo[5668]: pam_unix(sudo:session): session closed for user root\nSep 16 08:05:01 0916-072530-anrzknxi-10-172-214-67 CRON[5703]: pam_unix(cron:session): session opened for user root by (uid=0)\nSep 16 08:05:01 0916-072530-anrzknxi-10-172-214-67 CRON[5705]: (root) CMD (command -v debian-sa1 > /dev/null && debian-sa1 1 1)\nSep 16 08:05:01 0916-072530-anrzknxi-10-172-214-67 CRON[5703]: pam_unix(cron:session): session closed for user root\nSep 16 08:05:41 0916-072530-anrzknxi-10-172-214-67 sudo[5754]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/tail -f /var/log/elasticsearch/elasticsearch.log\nSep 16 08:05:41 0916-072530-anrzknxi-10-172-214-67 sudo[5754]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 08:09:01 0916-072530-anrzknxi-10-172-214-67 CRON[6086]: pam_unix(cron:session): session opened for user root by (uid=0)\nSep 16 08:09:01 0916-072530-anrzknxi-10-172-214-67 CRON[6087]: (root) CMD (  [ -x /usr/lib/php/sessionclean ] && if [ ! -d /run/systemd/system ]; then /usr/lib/php/sessionclean; fi)\nSep 16 08:09:01 0916-072530-anrzknxi-10-172-214-67 CRON[6086]: pam_unix(cron:session): session closed for user root\nSep 16 08:09:17 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Starting Clean php session files...\n-- Subject: A start job for unit phpsessionclean.service has begun execution\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit phpsessionclean.service has begun execution.\n-- \n-- The job identifier is 3808.\nSep 16 08:09:17 0916-072530-anrzknxi-10-172-214-67 systemd[1]: phpsessionclean.service: Succeeded.\n-- Subject: Unit succeeded\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- The unit phpsessionclean.service has successfully entered the 'dead' state.\nSep 16 08:09:17 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Finished Clean php session files.\n-- Subject: A start job for unit phpsessionclean.service has finished successfully\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit phpsessionclean.service has finished successfully.\n-- \n-- The job identifier is 3808.\nSep 16 08:14:16 0916-072530-anrzknxi-10-172-214-67 sudo[6676]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/systemctl status elasticsearch\nSep 16 08:14:16 0916-072530-anrzknxi-10-172-214-67 sudo[6676]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 08:14:17 0916-072530-anrzknxi-10-172-214-67 sudo[6676]: pam_unix(sudo:session): session closed for user root\nSep 16 08:15:01 0916-072530-anrzknxi-10-172-214-67 CRON[6758]: pam_unix(cron:session): session opened for user root by (uid=0)\nSep 16 08:15:01 0916-072530-anrzknxi-10-172-214-67 CRON[6759]: pam_unix(cron:session): session opened for user root by (uid=0)\nSep 16 08:15:01 0916-072530-anrzknxi-10-172-214-67 CRON[6760]: (root) CMD (command -v debian-sa1 > /dev/null && debian-sa1 1 1)\nSep 16 08:15:01 0916-072530-anrzknxi-10-172-214-67 CRON[6761]: (root) CMD (/databricks/spark/scripts/ganglia/save_snapshot.sh)\nSep 16 08:15:01 0916-072530-anrzknxi-10-172-214-67 CRON[6758]: pam_unix(cron:session): session closed for user root\nSep 16 08:15:01 0916-072530-anrzknxi-10-172-214-67 CRON[6759]: pam_unix(cron:session): session closed for user root\nSep 16 08:17:01 0916-072530-anrzknxi-10-172-214-67 CRON[7006]: pam_unix(cron:session): session opened for user root by (uid=0)\nSep 16 08:17:01 0916-072530-anrzknxi-10-172-214-67 CRON[7007]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)\nSep 16 08:17:01 0916-072530-anrzknxi-10-172-214-67 CRON[7006]: pam_unix(cron:session): session closed for user root\nSep 16 08:19:55 0916-072530-anrzknxi-10-172-214-67 sudo[7281]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/systemctl start elasticsearch\nSep 16 08:19:55 0916-072530-anrzknxi-10-172-214-67 sudo[7281]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 08:19:55 0916-072530-anrzknxi-10-172-214-67 sudo[7281]: pam_unix(sudo:session): session closed for user root\nSep 16 08:19:58 0916-072530-anrzknxi-10-172-214-67 sud\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n6 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.util.FileUtils.makeParentDirs(FileUtils.java:141)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory.createManager(RollingFileManager.java:734)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory.createManager(RollingFileManager.java:718)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.appender.AbstractManager.getManager(AbstractManager.java:144)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.appender.OutputStreamManager.getManager(OutputStreamManager.java:100)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.appender.rolling.RollingFileManager.getFileManager(RollingFileManager.java:217)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:135)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:62)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:124)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1138)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1063)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1055)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:664)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:258)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:304)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:621)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:285)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.elasticsearch.server@8.15.1/org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:250)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.elasticsearch.server@8.15.1/org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:129)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.elasticsearch.server@8.15.1/org.elasticsearch.bootstrap.Elasticsearch.initPhase1(Elasticsearch.java:136)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.elasticsearch.server@8.15.1/org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:71)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]: 2024-09-16 08:49:56,105 main ERROR Could not create plugin of type class org.apache.logging.log4j.core.appender.RollingFileAppender for element RollingFile: java.lang.IllegalStateException: ManagerFactory [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory@7c137fd5] unable to create manager for [/usr/share/elasticsearch/logs/elasticsearch_index_indexing_slowlog.json] with data [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$FactoryData@41294f8[pattern=/usr/share/elasticsearch/logs/elasticsearch_index_indexing_slowlog-%i.json.gz, append=true, bufferedIO=true, bufferSize=8192, policy=CompositeTriggeringPolicy(policies=[SizeBasedTriggeringPolicy(size=1073741824)]), strategy=DefaultRolloverStrategy(min=1, max=4, useMax=true), advertiseURI=null, layout=co.elastic.logging.log4j2.EcsLayout@2141a12, filePermissions=null, fileOwner=null]] java.lang.IllegalStateException: ManagerFactory [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$RollingFileManagerFactory@7c137fd5] unable to create manager for [/usr/share/elasticsearch/logs/elasticsearch_index_indexing_slowlog.json] with data [org.apache.logging.log4j.core.appender.rolling.RollingFileManager$FactoryData@41294f8[pattern=/usr/share/elasticsearch/logs/elasticsearch_index_indexing_slowlog-%i.json.gz, append=true, bufferedIO=true, bufferSize=8192, policy=CompositeTriggeringPolicy(policies=[SizeBasedTriggeringPolicy(size=1073741824)]), strategy=DefaultRolloverStrategy(min=1, max=4, useMax=true), advertiseURI=null, layout=co.elastic.logging.log4j2.EcsLayout@2141a12, filePermissions=null, fileOwner=null]]\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.appender.AbstractManager.getManager(AbstractManager.java:146)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.appender.OutputStreamManager.getManager(OutputStreamManager.java:100)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.appender.rolling.RollingFileManager.getFileManager(RollingFileManager.java:217)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:135)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.appender.RollingFileAppender$Builder.build(RollingFileAppender.java:62)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:124)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1138)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1063)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1055)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:664)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:258)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:304)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:621)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:285)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.elasticsearch.server@8.15.1/org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:250)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.elasticsearch.server@8.15.1/org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:129)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.elasticsearch.server@8.15.1/org.elasticsearch.bootstrap.Elasticsearch.initPhase1(Elasticsearch.java:136)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.elasticsearch.server@8.15.1/org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:71)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]: 2024-09-16 08:49:56,106 main ERROR Unable to invoke factory method in class org.apache.logging.log4j.core.appender.RollingFileAppender for element RollingFile: java.lang.IllegalStateException: No factory method found for class org.apache.logging.log4j.core.appender.RollingFileAppender java.lang.IllegalStateException: No factory method found for class org.apache.logging.log4j.core.appender.RollingFileAppender\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.findFactoryMethod(PluginBuilder.java:260)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:136)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1138)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1063)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1055)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:664)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:258)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:304)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:621)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.apache.logging.log4j.core@8.15.1/org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:285)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.elasticsearch.server@8.15.1/org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:250)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.elasticsearch.server@8.15.1/org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:129)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.elasticsearch.server@8.15.1/org.elasticsearch.bootstrap.Elasticsearch.initPhase1(Elasticsearch.java:136)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]:         at org.elasticsearch.server@8.15.1/org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:71)\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]: 2024-09-16 08:49:56,108 main ERROR Null object returned for RollingFile in Appenders.\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]: 2024-09-16 08:49:56,110 main ERROR Null object returned for RollingFile in Appenders.\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]: 2024-09-16 08:49:56,112 main ERROR Null object returned for RollingFile in Appenders.\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]: 2024-09-16 08:49:56,112 main ERROR Null object returned for RollingFile in Appenders.\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]: 2024-09-16 08:49:56,115 main ERROR Null object returned for RollingFile in Appenders.\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]: 2024-09-16 08:49:56,116 main ERROR Null object returned for RollingFile in Appenders.\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]: 2024-09-16 08:49:56,116 main ERROR Unable to locate appender \"rolling\" for logger config \"root\"\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]: 2024-09-16 08:49:56,117 main ERROR Unable to locate appender \"rolling_old\" for logger config \"root\"\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]: 2024-09-16 08:49:56,118 main ERROR Unable to locate appender \"index_indexing_slowlog_rolling\" for logger config \"index.indexing.slowlog.index\"\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]: 2024-09-16 08:49:56,119 main ERROR Unable to locate appender \"index_search_slowlog_rolling\" for logger config \"index.search.slowlog\"\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]: 2024-09-16 08:49:56,120 main ERROR Unable to locate appender \"deprecation_rolling\" for logger config \"org.elasticsearch.deprecation\"\nSep 16 08:49:56 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10660]: 2024-09-16 08:49:56,121 main ERROR Unable to locate appender \"audit_rolling\" for logger config \"org.elasticsearch.xpack.security.audit.logfile.LoggingAuditTrail\"\nSep 16 08:50:00 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10571]: ERROR: Elasticsearch did not exit normally - check the logs at /usr/share/elasticsearch/logs/elasticsearch.log\nSep 16 08:50:01 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10571]: ERROR: Elasticsearch died while starting up, with exit code 1\nSep 16 08:50:01 0916-072530-anrzknxi-10-172-214-67 systemd[1]: elasticsearch.service: Main process exited, code=exited, status=1/FAILURE\n-- Subject: Unit process exited\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- An ExecStart= process belonging to unit elasticsearch.service has exited.\n-- \n-- The process' exit code is 'exited' and its exit status is 1.\nSep 16 08:50:01 0916-072530-anrzknxi-10-172-214-67 systemd[1]: elasticsearch.service: Failed with result 'exit-code'.\n-- Subject: Unit failed\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- The unit elasticsearch.service has entered the 'failed' state with result 'exit-code'.\nSep 16 08:50:01 0916-072530-anrzknxi-10-172-214-67 sudo[10569]: pam_unix(sudo:session): session closed for user root\nSep 16 08:50:01 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Failed to start Elasticsearch.\n-- Subject: A start job for unit elasticsearch.service has failed\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit elasticsearch.service has finished with a failure.\n-- \n-- The job identifier is 7338 and the job result is failed.\nSep 16 08:51:00 0916-072530-anrzknxi-10-172-214-67 sudo[10814]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/systemctl status elasticsearch\nSep 16 08:51:00 0916-072530-anrzknxi-10-172-214-67 sudo[10814]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 08:51:00 0916-072530-anrzknxi-10-172-214-67 sudo[10814]: pam_unix(sudo:session): session closed for user root\nSep 16 08:51:27 0916-072530-anrzknxi-10-172-214-67 sudo[10857]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/cp /tmp/elasticsearch_cp.yml /etc/elasticsearch/elasticsearch.yml\nSep 16 08:51:27 0916-072530-anrzknxi-10-172-214-67 sudo[10857]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 08:51:27 0916-072530-anrzknxi-10-172-214-67 sudo[10857]: pam_unix(sudo:session): session closed for user root\nSep 16 08:51:51 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: local_clock: ntp_loopfilter.c line 818: ntp_adjtime: Operation not permitted\nSep 16 08:51:57 0916-072530-anrzknxi-10-172-214-67 sudo[10902]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/systemctl restart elasticsearch\nSep 16 08:51:57 0916-072530-anrzknxi-10-172-214-67 sudo[10902]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 08:51:57 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Starting Elasticsearch...\n-- Subject: A start job for unit elasticsearch.service has begun execution\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit elasticsearch.service has begun execution.\n-- \n-- The job identifier is 7554.\nSep 16 08:52:10 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10904]: Sep 16, 2024 8:52:10 AM sun.util.locale.provider.LocaleProviderAdapter <clinit>\nSep 16 08:52:10 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[10904]: WARNING: COMPAT locale provider will be removed in a future release\nSep 16 08:53:01 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Started Elasticsearch.\n-- Subject: A start job for unit elasticsearch.service has finished successfully\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit elasticsearch.service has finished successfully.\n-- \n-- The job identifier is 7554.\nSep 16 08:53:01 0916-072530-anrzknxi-10-172-214-67 sudo[10902]: pam_unix(sudo:session): session closed for user root\nSep 16 08:53:18 0916-072530-anrzknxi-10-172-214-67 sudo[11210]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/systemctl status elasticsearch\nSep 16 08:53:18 0916-072530-anrzknxi-10-172-214-67 sudo[11210]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 08:53:19 0916-072530-anrzknxi-10-172-214-67 sudo[11210]: pam_unix(sudo:session): session closed for user root\nSep 16 08:54:04 0916-072530-anrzknxi-10-172-214-67 ntpd[84]: local_clock: ntp_loopfilter.c line 818: ntp_adjtime: Operation not permitted\nSep 16 08:55:02 0916-072530-anrzknxi-10-172-214-67 CRON[11360]: pam_unix(cron:session): session opened for user root by (uid=0)\nSep 16 08:55:02 0916-072530-anrzknxi-10-172-214-67 CRON[11361]: (root) CMD (command -v debian-sa1 > /dev/null && debian-sa1 1 1)\nSep 16 08:55:02 0916-072530-anrzknxi-10-172-214-67 CRON[11360]: pam_unix(cron:session): session closed for user root\nSep 16 08:59:55 0916-072530-anrzknxi-10-172-214-67 sudo[11874]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/rm /etc/elasticsearch/elasticsearch.yml\nSep 16 08:59:55 0916-072530-anrzknxi-10-172-214-67 sudo[11874]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 08:59:55 0916-072530-anrzknxi-10-172-214-67 sudo[11874]: pam_unix(sudo:session): session closed for user root\nSep 16 09:00:00 0916-072530-anrzknxi-10-172-214-67 sudo[11878]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/cp /tmp/elasticsearch_new.yml /etc/elasticsearch/elasticsearch.yml\nSep 16 09:00:00 0916-072530-anrzknxi-10-172-214-67 sudo[11878]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 09:00:00 0916-072530-anrzknxi-10-172-214-67 sudo[11878]: pam_unix(sudo:session): session closed for user root\nSep 16 09:00:01 0916-072530-anrzknxi-10-172-214-67 CRON[11883]: pam_unix(cron:session): session opened for user root by (uid=0)\nSep 16 09:00:01 0916-072530-anrzknxi-10-172-214-67 CRON[11882]: pam_unix(cron:session): session opened for user root by (uid=0)\nSep 16 09:00:01 0916-072530-anrzknxi-10-172-214-67 CRON[11884]: (root) CMD (/databricks/spark/scripts/ganglia/remove_old_ganglia_rrds.sh 24 &> /var/log/rrd_cleanup.log)\nSep 16 09:00:01 0916-072530-anrzknxi-10-172-214-67 CRON[11885]: (root) CMD (/databricks/spark/scripts/ganglia/save_snapshot.sh)\nSep 16 09:00:01 0916-072530-anrzknxi-10-172-214-67 CRON[11882]: pam_unix(cron:session): session closed for user root\nSep 16 09:00:01 0916-072530-anrzknxi-10-172-214-67 CRON[11883]: pam_unix(cron:session): session closed for user root\nSep 16 09:00:05 0916-072530-anrzknxi-10-172-214-67 sudo[11936]:     root : TTY=unknown ; PWD=/databricks/driver ; USER=root ; COMMAND=/usr/bin/systemctl restart elasticsearch\nSep 16 09:00:05 0916-072530-anrzknxi-10-172-214-67 sudo[11936]: pam_unix(sudo:session): session opened for user root by (uid=0)\nSep 16 09:00:05 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Stopping Elasticsearch...\n-- Subject: A stop job for unit elasticsearch.service has begun execution\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A stop job for unit elasticsearch.service has begun execution.\n-- \n-- The job identifier is 8286.\nSep 16 09:00:08 0916-072530-anrzknxi-10-172-214-67 systemd[1]: elasticsearch.service: Succeeded.\n-- Subject: Unit succeeded\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- The unit elasticsearch.service has successfully entered the 'dead' state.\nSep 16 09:00:08 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Stopped Elasticsearch.\n-- Subject: A stop job for unit elasticsearch.service has finished\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A stop job for unit elasticsearch.service has finished.\n-- \n-- The job identifier is 8286 and the job result is done.\nSep 16 09:00:08 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Starting Elasticsearch...\n-- Subject: A start job for unit elasticsearch.service has begun execution\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit elasticsearch.service has begun execution.\n-- \n-- The job identifier is 8286.\nSep 16 09:00:23 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[11953]: Sep 16, 2024 9:00:23 AM sun.util.locale.provider.LocaleProviderAdapter <clinit>\nSep 16 09:00:23 0916-072530-anrzknxi-10-172-214-67 systemd-entrypoint[11953]: WARNING: COMPAT locale provider will be removed in a future release\nSep 16 09:01:04 0916-072530-anrzknxi-10-172-214-67 systemd[1]: Started Elasticsearch.\n-- Subject: A start job for unit elasticsearch.service has finished successfully\n-- Defined-By: systemd\n-- Support: http://www.ubuntu.com/support\n-- \n-- A start job for unit elasticsearch.service has finished successfully.\n-- \n-- The job identifier is 8286.\nSep 16 09:01:05 0916-072530-anrzknxi-10-172-214-67 sudo[11936]: pam_unix(sudo:session): session closed for user root\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "\n",
    "journalctl -xe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16996dbf-0e9f-4295-9edf-5083d7c605a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "netstat | grep 9200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa1c019b-a582-45fc-b0ad-7df1f2d42fce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Test elasticsearch with curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be9d9edb-98c7-4f7a-b043-c40785f9555e",
     "showTitle": true,
     "title": "curl - check"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   559  100   559    0     0  15971      0 --:--:-- --:--:-- --:--:-- 15971\n{\n  \"name\" : \"0916-072530-anrzknxi-10-172-214-67\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"cluster_uuid\" : \"ZuQf3GKpQFiS_NrD0llKLw\",\n  \"version\" : {\n    \"number\" : \"8.15.1\",\n    \"build_flavor\" : \"default\",\n    \"build_type\" : \"deb\",\n    \"build_hash\" : \"253e8544a65ad44581194068936f2a5d57c2c051\",\n    \"build_date\" : \"2024-09-02T22:04:47.310170297Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"9.11.1\",\n    \"minimum_wire_compatibility_version\" : \"7.17.0\",\n    \"minimum_index_compatibility_version\" : \"7.0.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "curl -X GET \"localhost:9200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10048e0d-dc06-4c8a-b1f3-29bbcb66d70a",
     "showTitle": true,
     "title": "curl - get cluster health"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   389  100   389    0     0   2080      0 --:--:-- --:--:-- --:--:--  2080\n100   389  100   389    0     0   2080      0 --:--:-- --:--:-- --:--:--  2069\n{\"cluster_name\":\"elasticsearch\",\"status\":\"green\",\"timed_out\":false,\"number_of_nodes\":1,\"number_of_data_nodes\":1,\"active_primary_shards\":0,\"active_shards\":0,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":0,\"delayed_unassigned_shards\":0,\"number_of_pending_tasks\":0,\"number_of_in_flight_fetch\":0,\"task_max_waiting_in_queue_millis\":0,\"active_shards_percent_as_number\":100.0}"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "\n",
    "curl -X GET \"localhost:9200/_cluster/health\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91f01e2d-5e5a-4b74-a6a4-f1a0efd1e713",
     "showTitle": true,
     "title": "curl - create index ?"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100    86  100    86    0     0     98      0 --:--:-- --:--:-- --:--:--    98\n100    86  100    86    0     0     98      0 --:--:-- --:--:-- --:--:--    98\n{\n  \"acknowledged\" : true,\n  \"shards_acknowledged\" : true,\n  \"index\" : \"curl_index\"\n}\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "\n",
    "curl -X PUT \"localhost:9200/curl_index?pretty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "316735c9-949b-40ad-977a-ee193317a8ff",
     "showTitle": true,
     "title": "curl - insert values"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100  1228    0  1044  100   184   2367    417 --:--:-- --:--:-- --:--:--  2784\n{\n  \"errors\" : false,\n  \"took\" : 400,\n  \"items\" : [\n    {\n      \"index\" : {\n        \"_index\" : \"curl_index\",\n        \"_id\" : \"1\",\n        \"_version\" : 1,\n        \"result\" : \"created\",\n        \"_shards\" : {\n          \"total\" : 2,\n          \"successful\" : 1,\n          \"failed\" : 0\n        },\n        \"_seq_no\" : 0,\n        \"_primary_term\" : 1,\n        \"status\" : 201\n      }\n    },\n    {\n      \"index\" : {\n        \"_index\" : \"curl_index\",\n        \"_id\" : \"2\",\n        \"_version\" : 1,\n        \"result\" : \"created\",\n        \"_shards\" : {\n          \"total\" : 2,\n          \"successful\" : 1,\n          \"failed\" : 0\n        },\n        \"_seq_no\" : 1,\n        \"_primary_term\" : 1,\n        \"status\" : 201\n      }\n    },\n    {\n      \"index\" : {\n        \"_index\" : \"curl_index\",\n        \"_id\" : \"3\",\n        \"_version\" : 1,\n        \"result\" : \"created\",\n        \"_shards\" : {\n          \"total\" : 2,\n          \"successful\" : 1,\n          \"failed\" : 0\n        },\n        \"_seq_no\" : 2,\n        \"_primary_term\" : 1,\n        \"status\" : 201\n      }\n    }\n  ]\n}\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "\n",
    "curl -X POST \"localhost:9200/curl_index/_bulk?pretty\" -H 'Content-Type: application/json' -d'\n",
    "{ \"index\": { \"_id\": \"1\" } }\n",
    "{ \"name\": \"Bilbo\", \"age\": 50 }\n",
    "{ \"index\": { \"_id\": \"2\" } }\n",
    "{ \"name\": \"Gandalf\", \"age\": 1000 }\n",
    "{ \"index\": { \"_id\": \"3\" } }\n",
    "{ \"name\": \"Thorin\", \"age\": 195 }\n",
    "'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231131f3-ca9b-43f6-b75a-86a9291e01a5",
     "showTitle": true,
     "title": "curl - search all"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   799    0   799    0     0   4700      0 --:--:-- --:--:-- --:--:--  4727\n{\n  \"took\" : 157,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 1,\n    \"successful\" : 1,\n    \"skipped\" : 0,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : {\n      \"value\" : 3,\n      \"relation\" : \"eq\"\n    },\n    \"max_score\" : 1.0,\n    \"hits\" : [\n      {\n        \"_index\" : \"curl_index\",\n        \"_id\" : \"1\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"name\" : \"Bilbo\",\n          \"age\" : 50\n        }\n      },\n      {\n        \"_index\" : \"curl_index\",\n        \"_id\" : \"2\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"name\" : \"Gandalf\",\n          \"age\" : 1000\n        }\n      },\n      {\n        \"_index\" : \"curl_index\",\n        \"_id\" : \"3\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"name\" : \"Thorin\",\n          \"age\" : 195\n        }\n      }\n    ]\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "\n",
    "curl -X GET \"localhost:9200/curl_index/_search?pretty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57b8c48d-b556-4d40-98c8-0f1da84ce8fc",
     "showTitle": true,
     "title": "curl - content"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   179  100   179    0     0   6629      0 --:--:-- --:--:-- --:--:--  6629\n{\n  \"_index\" : \"curl_index\",\n  \"_id\" : \"1\",\n  \"_version\" : 1,\n  \"_seq_no\" : 0,\n  \"_primary_term\" : 1,\n  \"found\" : true,\n  \"_source\" : {\n    \"name\" : \"Bilbo\",\n    \"age\" : 50\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "\n",
    "curl -X GET \"localhost:9200/curl_index/_doc/1?pretty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2589b62b-cd7b-4093-80dc-7b0e568c00b7",
     "showTitle": true,
     "title": "curl - remove 1st entry"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   204  100   204    0     0   5828      0 --:--:-- --:--:-- --:--:--  5828\n{\n  \"_index\" : \"curl_index\",\n  \"_id\" : \"1\",\n  \"_version\" : 2,\n  \"result\" : \"deleted\",\n  \"_shards\" : {\n    \"total\" : 2,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"_seq_no\" : 3,\n  \"_primary_term\" : 1\n}\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "\n",
    "curl -X DELETE \"localhost:9200/curl_index/_doc/1?pretty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd76703-36d3-4a7d-af23-5cf5280f676f",
     "showTitle": true,
     "title": "curl - content"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   621    0   621    0     0  32684      0 --:--:-- --:--:-- --:--:-- 32684\n{\n  \"took\" : 4,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 1,\n    \"successful\" : 1,\n    \"skipped\" : 0,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : {\n      \"value\" : 2,\n      \"relation\" : \"eq\"\n    },\n    \"max_score\" : 1.0,\n    \"hits\" : [\n      {\n        \"_index\" : \"curl_index\",\n        \"_id\" : \"2\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"name\" : \"Gandalf\",\n          \"age\" : 1000\n        }\n      },\n      {\n        \"_index\" : \"curl_index\",\n        \"_id\" : \"3\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"name\" : \"Thorin\",\n          \"age\" : 195\n        }\n      }\n    ]\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "\n",
    "curl -X GET \"localhost:9200/curl_index/_search/?pretty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f76960-fad3-4414-976f-40d8dac21a35",
     "showTitle": true,
     "title": "curl - search starts with value"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   250    0   250    0     0  16666      0 --:--:-- --:--:-- --:--:-- 16666\n{\"took\":4,\"timed_out\":false,\"_shards\":{\"total\":1,\"successful\":1,\"skipped\":0,\"failed\":0},\"hits\":{\"total\":{\"value\":1,\"relation\":\"eq\"},\"max_score\":1.0,\"hits\":[{\"_index\":\"curl_index\",\"_id\":\"2\",\"_score\":1.0,\"_source\":{ \"name\": \"Gandalf\", \"age\": 1000 }}]}}"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "\n",
    "curl -X GET \"localhost:9200/curl_index/_search?q=Gan*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95470ca1-ea8c-48a9-9fd1-f232741d6e4e",
     "showTitle": true,
     "title": "curl - search fix value"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   248    0   248    0     0  17714      0 --:--:-- --:--:-- --:--:-- 17714\n{\"took\":6,\"timed_out\":false,\"_shards\":{\"total\":1,\"successful\":1,\"skipped\":0,\"failed\":0},\"hits\":{\"total\":{\"value\":1,\"relation\":\"eq\"},\"max_score\":1.0,\"hits\":[{\"_index\":\"curl_index\",\"_id\":\"3\",\"_score\":1.0,\"_source\":{ \"name\": \"Thorin\", \"age\": 195 }}]}}"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "\n",
    "curl -X GET \"localhost:9200/curl_index/_search?q=195\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc2e7250-85d4-483a-b9e7-231537472620",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Test elastic search in databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b29ea68-c5f4-4a02-bec8-3301dabf90bb",
     "showTitle": true,
     "title": "Test connectivity to your Elasticsearch cluster"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to localhost 9200 port [tcp/*] succeeded!\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "nc -vz localhost 9200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4db7c0f1-11be-4238-b4c0-95c2eec8590d",
     "showTitle": true,
     "title": "Make trivial test dataframe"
    }
   },
   "outputs": [],
   "source": [
    "people = spark.createDataFrame( [ (\"Bilbo\",     50), \n",
    "                                  (\"Gandalf\", 1000), \n",
    "                                  (\"Thorin\",   195),  \n",
    "                                  (\"Balin\",    178), \n",
    "                                  (\"Kili\",      77),\n",
    "                                  (\"Dwalin\",   169), \n",
    "                                  (\"Oin\",      167), \n",
    "                                  (\"Gloin\",    158), \n",
    "                                  (\"Fili\",      82), \n",
    "                                  (\"Bombur\",  None)\n",
    "                                ], \n",
    "                                [\"name\", \"age\"] \n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90ec043a-4350-459b-8640-260bf9309dca",
     "showTitle": true,
     "title": "Write to Elasticsearch"
    }
   },
   "outputs": [],
   "source": [
    "# Overwrite the data each time\n",
    "\n",
    "# NOTE: We **must** set the es.nodes.wan.only property to 'true' so that the connector will connect to the node(s) specified by the `es.nodes` parameter.\n",
    "#       Without this setting, the ES connector will try to discover ES nodes on the network using a broadcast ping, which won't work.\n",
    "#       We want to connect to the node(s) specified in `es.nodes`.\n",
    "\n",
    "es_read_conf = {\n",
    "    \"es.nodes\": \"localhost\",\n",
    "    \"es.port\": \"9200\",\n",
    "    \"es.index.auto.create\": \"true\",\n",
    "    \"es.nodes.wan.only\": \"true\",  # Enable for WAN/Cloud instances\n",
    "}\n",
    "\n",
    "( people.write\n",
    "  .format( \"org.elasticsearch.spark.sql\" )\n",
    "  .options(**es_read_conf)\n",
    "  .mode(\"overwrite\")  # Optional: Overwrite existing data if needed\n",
    "  .save(\"databricks_index\")  # Ensure index name is correct\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cdba138-252d-41fa-a0a6-b5eea8d5994f",
     "showTitle": true,
     "title": "Read from Elasticsearch - ALL"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>age</th><th>name</th></tr></thead><tbody><tr><td>50</td><td>Bilbo</td></tr><tr><td>169</td><td>Dwalin</td></tr><tr><td>178</td><td>Balin</td></tr><tr><td>77</td><td>Kili</td></tr><tr><td>167</td><td>Oin</td></tr><tr><td>82</td><td>Fili</td></tr><tr><td>null</td><td>Bombur</td></tr><tr><td>195</td><td>Thorin</td></tr><tr><td>1000</td><td>Gandalf</td></tr><tr><td>158</td><td>Gloin</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         50,
         "Bilbo"
        ],
        [
         169,
         "Dwalin"
        ],
        [
         178,
         "Balin"
        ],
        [
         77,
         "Kili"
        ],
        [
         167,
         "Oin"
        ],
        [
         82,
         "Fili"
        ],
        [
         null,
         "Bombur"
        ],
        [
         195,
         "Thorin"
        ],
        [
         1000,
         "Gandalf"
        ],
        [
         158,
         "Gloin"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE: We **must** set the es.nodes.wan.only property to 'true' so that the connector will connect to the node(s) specified by the `es.nodes` parameter.\n",
    "#       Without this setting, the ES connector will try to discover ES nodes on the network using a broadcast ping, which won't work.\n",
    "#       We want to connect to the node(s) specified in `es.nodes`.\n",
    "df = (spark.read\n",
    "      .format( \"org.elasticsearch.spark.sql\" )\n",
    "      .option( \"es.nodes\",   \"localhost\" )\n",
    "      .option( \"es.port\",    9200     )\n",
    "      #.option( \"es.net.ssl\", ssl      )\n",
    "      .option( \"es.nodes.wan.only\", \"true\" )\n",
    "      #.option(\"es.query\", \"?q=*o*\") # filter on all columns for contains \"o\"\n",
    "      #.option(\"es.query\", '{\"query\": {\"term\": {\"age\": 77}}}')  # filter on equals 77\n",
    "      #.option(\"es.query\", '{\"query\": {\"range\": {\"age\": {\"gt\": 77}}}}')  # Filter where 'age' is greater than 77\n",
    "      .load( f\"databricks_index\" )#.save( f\"{index}\" )\n",
    "     )\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ded198-e34d-419a-b545-a994d2b98eb6",
     "showTitle": true,
     "title": "Read from Elasticsearch - contains \"o\""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>age</th><th>name</th></tr></thead><tbody><tr><td>50</td><td>Bilbo</td></tr><tr><td>167</td><td>Oin</td></tr><tr><td>null</td><td>Bombur</td></tr><tr><td>195</td><td>Thorin</td></tr><tr><td>158</td><td>Gloin</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         50,
         "Bilbo"
        ],
        [
         167,
         "Oin"
        ],
        [
         null,
         "Bombur"
        ],
        [
         195,
         "Thorin"
        ],
        [
         158,
         "Gloin"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE: We **must** set the es.nodes.wan.only property to 'true' so that the connector will connect to the node(s) specified by the `es.nodes` parameter.\n",
    "#       Without this setting, the ES connector will try to discover ES nodes on the network using a broadcast ping, which won't work.\n",
    "#       We want to connect to the node(s) specified in `es.nodes`.\n",
    "df = (spark.read\n",
    "      .format( \"org.elasticsearch.spark.sql\" )\n",
    "      .option( \"es.nodes\",   \"localhost\" )\n",
    "      .option( \"es.port\",    9200     )\n",
    "      #.option( \"es.net.ssl\", ssl      )\n",
    "      .option( \"es.nodes.wan.only\", \"true\" )\n",
    "      .option(\"es.query\", \"?q=*o*\") # filter on all columns for contains \"o\"\n",
    "      #.option(\"es.query\", '{\"query\": {\"term\": {\"age\": 77}}}')  # filter on equals 77\n",
    "      #.option(\"es.query\", '{\"query\": {\"range\": {\"age\": {\"gt\": 77}}}}')  # Filter where 'age' is greater than 77\n",
    "      .load( f\"databricks_index\" )#.save( f\"{index}\" )\n",
    "     )\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "895fb102-a0ed-4dee-a80d-0dd408054be2",
     "showTitle": true,
     "title": "Read from Elasticsearch - age equals 77"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>age</th><th>name</th></tr></thead><tbody><tr><td>77</td><td>Kili</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         77,
         "Kili"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE: We **must** set the es.nodes.wan.only property to 'true' so that the connector will connect to the node(s) specified by the `es.nodes` parameter.\n",
    "#       Without this setting, the ES connector will try to discover ES nodes on the network using a broadcast ping, which won't work.\n",
    "#       We want to connect to the node(s) specified in `es.nodes`.\n",
    "df = (spark.read\n",
    "      .format( \"org.elasticsearch.spark.sql\" )\n",
    "      .option( \"es.nodes\",   \"localhost\" )\n",
    "      .option( \"es.port\",    9200     )\n",
    "      #.option( \"es.net.ssl\", ssl      )\n",
    "      .option( \"es.nodes.wan.only\", \"true\" )\n",
    "      #.option(\"es.query\", \"?q=*o*\") # filter on all columns for contains \"o\"\n",
    "      .option(\"es.query\", '{\"query\": {\"term\": {\"age\": 77}}}')  # filter on age equals 77\n",
    "      #.option(\"es.query\", '{\"query\": {\"range\": {\"age\": {\"gt\": 77}}}}')  # Filter where 'age' is greater than 77\n",
    "      .load( f\"databricks_index\" )#.save( f\"{index}\" )\n",
    "     )\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9e4cfa0-e6f8-45a2-a3bf-1a2c5abba3db",
     "showTitle": true,
     "title": "Read from Elasticsearch - age greater than 77"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>age</th><th>name</th></tr></thead><tbody><tr><td>169</td><td>Dwalin</td></tr><tr><td>178</td><td>Balin</td></tr><tr><td>167</td><td>Oin</td></tr><tr><td>82</td><td>Fili</td></tr><tr><td>195</td><td>Thorin</td></tr><tr><td>1000</td><td>Gandalf</td></tr><tr><td>158</td><td>Gloin</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         169,
         "Dwalin"
        ],
        [
         178,
         "Balin"
        ],
        [
         167,
         "Oin"
        ],
        [
         82,
         "Fili"
        ],
        [
         195,
         "Thorin"
        ],
        [
         1000,
         "Gandalf"
        ],
        [
         158,
         "Gloin"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE: We **must** set the es.nodes.wan.only property to 'true' so that the connector will connect to the node(s) specified by the `es.nodes` parameter.\n",
    "#       Without this setting, the ES connector will try to discover ES nodes on the network using a broadcast ping, which won't work.\n",
    "#       We want to connect to the node(s) specified in `es.nodes`.\n",
    "df = (spark.read\n",
    "      .format( \"org.elasticsearch.spark.sql\" )\n",
    "      .option( \"es.nodes\",   \"localhost\" )\n",
    "      .option( \"es.port\",    9200     )\n",
    "      #.option( \"es.net.ssl\", ssl      )\n",
    "      .option( \"es.nodes.wan.only\", \"true\" )\n",
    "      #.option(\"es.query\", \"?q=*o*\") # filter on all columns for contains \"o\"\n",
    "      #.option(\"es.query\", '{\"query\": {\"term\": {\"age\": 77}}}')  # filter on equals 77\n",
    "      .option(\"es.query\", '{\"query\": {\"range\": {\"age\": {\"gt\": 77}}}}')  # Filter where 'age' is greater than 77\n",
    "      .load( f\"databricks_index\" )#.save( f\"{index}\" )\n",
    "     )\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6673d86d-204f-427c-8840-6e90d91d37f8",
     "showTitle": true,
     "title": "Write to Delta"
    }
   },
   "outputs": [],
   "source": [
    "# Creates a Delta table called table_name\n",
    "#df.write.format(\"delta\").saveAsTable(table_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 787991335380521,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "elasticsearch Notebook 2024-09-16 09:27:02",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
